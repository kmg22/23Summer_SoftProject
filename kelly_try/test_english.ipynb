{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "happy = \"\"\n",
    "top = -1\n",
    "    \n",
    "# Define actions and corresponding labels\n",
    "actions = ['a', 'b', 'c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','space','backspace']\n",
    "seq_length = 30\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "seq = []\n",
    "action_seq = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    img0 = img.copy()\n",
    "    \n",
    "    \n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # cv2.putText(img,f'hi', org=[50,50], fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "    \n",
    "\n",
    "    if result.multi_hand_landmarks is not None:\n",
    "        \n",
    "    \n",
    "        for res in result.multi_hand_landmarks:\n",
    "            joint = np.zeros((21, 4))\n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "                \n",
    "                \n",
    "\n",
    "            v1 = joint[[0, 1, 2, 3, 0, 5, 6, 7, 0, 9, 10, 11, 0, 13, 14, 15, 0, 17, 18, 19], :3]\n",
    "            v2 = joint[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], :3]\n",
    "            v = v2 - v1\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "            angle = np.arccos(np.einsum('nt,nt->n', v[[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 18], :],\n",
    "                                        v[[1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 18, 19], :]))\n",
    "            angle = np.degrees(angle)\n",
    "\n",
    "            d = np.concatenate([joint.flatten(), angle])\n",
    "            seq.append(d)\n",
    "\n",
    "            mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            if len(seq) < seq_length:\n",
    "                continue\n",
    "\n",
    "            input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "            y_pred = model.predict(input_data).squeeze()\n",
    "\n",
    "            i_pred = int(np.argmax(y_pred))\n",
    "            conf = y_pred[i_pred]\n",
    "            \n",
    "#             cv2.putText(img, 'yes!!', org=[25,50],\n",
    "#                     fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "            if conf < 0.95:\n",
    "                continue\n",
    "\n",
    "            action = actions[i_pred]\n",
    "            action_seq.append(action)\n",
    "\n",
    "            if len(action_seq) < 3:\n",
    "                continue\n",
    "\n",
    "            this_action = '?'\n",
    "            if action_seq[-1] == action_seq[-2] == action_seq[-3]: # 정확한 액션이라면\n",
    "                this_action = action\n",
    "                \n",
    "                #org=(int(res.landmark[0].x * img.shape[1])\n",
    "                \n",
    "            \n",
    "            print(this_action)\n",
    "            \n",
    "            if cv2.waitKey(1) == ord('a'):\n",
    "                if this_action == 'space':\n",
    "                    this_action = \" \"\n",
    "                    happy = happy + this_action\n",
    "                    top +=1\n",
    "                    \n",
    "                if this_action =='backspace':\n",
    "                    happy = happy[:-1]\n",
    "                    print(happy)\n",
    "                    \n",
    "                else:\n",
    "                    happy = happy + this_action.upper() # upper() 대문자 변환 함수\n",
    "                    top += 1\n",
    "                \n",
    "            cv2.putText(img, f'{this_action.upper()}', org=[25,50],\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "          \n",
    "            # happy = ''.join(str(_) for _ in happy)\n",
    "            print(type(happy))\n",
    "             \n",
    "            cv2.putText(img , happy , org=[25,450], fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)        \n",
    "            cv2.imshow('img', img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
